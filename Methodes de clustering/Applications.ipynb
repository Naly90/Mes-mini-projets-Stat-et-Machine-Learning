{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises of application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a set of data on the customers of a shopping mall such as their gender, age, annual income and spending score. The objective of this assignment is to use the clustering methods outlined in the slides to determine which categories of customers have better spending scores. To download the data set click on the link <a href=\"https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\">Mall Customers</a>  \n",
    "\n",
    "1. Make scatter plots of the different variables by differentiating each point in space by the color of the sex of the individual. You will use the `scatter()` function of matplotlib. \n",
    "2. The graphical results show that the formation of several groups according to certain variables. What are these variables ? \n",
    "3. Use the Elbow method in the k-means to find the optimal number of clusters. To do this, you need to determine the sums of the squares of the distances (total inertia) found in the set of methods of the `KMeans()` function. \n",
    "4. You will use the different metrics stated at the end of the exercise. You will use the `sklearn.metrics` module to import these evaluation measures and for similarity measures like the Euclidean distance you will use `sklearn.metrics.pairwise`\n",
    "5. For each of these methods, you will compare the results of each method with that of the k-means. \n",
    "\n",
    "before all thing, don't forget to make scale on the data (you will use the `StandardScaler()` imported for `sklearn.preprocessing`). \n",
    "\n",
    "For this, you will use the following clustering techniques:\n",
    "\n",
    "1. Partition-based clustering\n",
    "\n",
    "     * K-means  \n",
    "          - For this method, make the graphic also the clusters and centroids\n",
    "          - Determine the total inertia and the interpretation (The inertia represents the sum of the squares of the distances and is contained in the set of parameters of the KMeans method of the module `sklearn.cluster`)\n",
    "          - Evaluate this algorithm with the different unsupervised evaluation measures stated at the end of the exercise. Use the package `sklearn.metrics` for import the different metrics.\n",
    "          - Plot the graph of silhoutte and The vertical line for average silhouette score of all the values. To do this, Create a subplot with 1 row and 2 columns. Compute the silhouette scores for each sample with the function `silhoutte_samples()`. Aggregate the silhouette scores for samples belonging to cluster, and sort them. Then draw the vertical line for average silhouette score of all the values. Then draw the second graph which represents the different clusters.\n",
    "\n",
    "     * K-medoids\n",
    "          - Apply this method on the data and make the graphical representation of clusters and medoids. You must install the `Kmedoids` module with the command `!pip install Kmedoids`, You will use the `method Kmedoids.pam()` on the dissimilarity matrix that you must calculate with the appropriate distance. specify the default parameters in your function.\n",
    "          - calculed intra-medoid and inter-medoid silhoutte coeficient. Use the `kmedoids.silhouette()` functions to determine the intramedoid silhouette index and `kmedoids.medoid_silhouette()` to determine the intermedoid silhouette index. \n",
    "          - Plot the validation indices as a function of k clusters. For that, you should create 3 lists in which will be inserted these k values for each validation index.\n",
    "          - Make the graphic also the clusters and centroids. You will use the `plt.scatter()` function of `matplotlib.pyplot`.\n",
    "          - Analysed the results \n",
    "\n",
    "     * Fuzzy C-means\n",
    "          - apply this method on the data and the cluster centers and the membership matrix. Use the `! pip install fuzzy-c-means` command to install the library, import the `FCM` function from the `fcmeans` package. Specify the number of ideal clusters, the number of iterations and the `random_state` and train the model on the standardized data. Obtain the cluster centers and the membership matrix from the `centers` function of the model. Assign each data point to the cluster with the highest membership with the constructor `u` of model (`model.fit().u`) to retrieve only the values. Assign each data point to the cluster with the highest membership using the function `np.armax()` which returns the indices of the maximum values along an axis.\n",
    "          - For this method, make the graphic also the clusters and centroids.\n",
    "          - Evaluate this algorithm with the differents similarities measures unsupervised such as the partition coefficient and partition entropy coefficient \n",
    "          - determine the silhoutte scores, calinski harabasz score and davies bouldin score and interpreted interpret these results\n",
    "\n",
    "     * Spectral clustering\n",
    "          - determine the matrices of the non-normalized and normalized Laplacian\n",
    "          - convert this normalized matrix into a dataframe\n",
    "          - use Reducing the dimensions of the data and given the principal components. \n",
    "          - apply the spectral clustering on the data and visualy the clsuters. You will use `PCA` from de library `sklearn.decomposition`. You will use the Gaussian similarity function associated with the Euclidean distance as a parameter affinity matrix. You will choose the kernel of the similarity function $\\sigma$ according to the validation index of the cluster (the number of classes according to the silhoutte score)\n",
    "          - analys each clusters and plot the validation indices as a function of the number of nearest neighbors\n",
    "          - plot Validation indices as a function of the number k of classes\n",
    "\n",
    "2. Hierarchical Clustering\n",
    "\n",
    "     * Agglomerative clustering\n",
    "          - Determine the dissimilarity matrix with the Ward distance and \n",
    "          - plot the dendrogram for the linkage_array containing the distances between clusters. You will use the fonctions `dendrogram` and `ward` from `scipy.cluster.hierarchy`\n",
    "          -  represents the points that belong to the clusters and visualize the clusters\n",
    "          - determine the silhoutte scores, calinski harabasz score and davies bouldin score and interpreted interpret these results\n",
    "\n",
    "3. Density-based Clustering\n",
    "\n",
    "     * DBSCAN\n",
    "          - build the number of clusters in the labels, ignoring noise if it is present. You will use the fonction `DBSCAN` from `sklearn.cluster`\n",
    "          - make the graphic also the clusters and the number of noise \n",
    "          - give a graphical representation of the clusters and the number of noise with eps and min_samples\n",
    "          - determine the silhoutte scores, calinski harabasz score and davies bouldin score and interpreted interpret these results\n",
    "     \n",
    "\n",
    "**Unsupervised measures** : \n",
    " \n",
    "     - Silhouette Coefficient\n",
    "     - Calinski-Harabasz Index\n",
    "     - Davies-Bouldin Index    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
